---
layout: post
title: AIreflections#4 - bias-variance tradeoff
---

Let us revisit the bias-variance tradeoff - a topic of timeless intrigue in statistics and machine learning.

The bias-variance tradeoff is a fundamental concept in machine learning that refers to the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:

- Bias is the error introduced by approximating a complex real-world problem with a simpler model. Models with high bias tend to underfit the training data, making overly simplistic assumptions [[2](#ref-2)] [[4](#ref-4)] [[11](#ref-11)].

- Variance is the error introduced by the model's sensitivity to small fluctuations in the training set. Models with high variance tend to overfit the training data, modeling the random noise instead of the intended outputs [[2](#ref-2)] [[4](#ref-4)] [[11](#ref-11)].

## Relationship Between Bias and Variance

There is an inverse relationship between bias and variance [[2](#ref-2)] [[6](#ref-6)]:

- Models with low bias tend to have high variance. They are more complex and flexible, so they fit the training data very closely but don't generalize well to new data (overfitting) [[2](#ref-2)] [[4](#ref-4)] [[10](#ref-10)].

- Models with high bias tend to have low variance. They are simpler, so they may underfit the training data but they are less sensitive to the noise and fluctuations in it [[2](#ref-2)] [[4](#ref-4)] [[10](#ref-10)].

The goal is to find the sweet spot that balances bias and variance and minimizes the total error [[6](#ref-6)] [[11](#ref-11)]. Too simple a model has high bias and underfits, while too complex a model has high variance and overfits [[4](#ref-4)] [[13](#ref-13)]. The optimal model complexity is somewhere in between.

## Visualizing the Tradeoff

The bias-variance tradeoff can be visualized by plotting both bias and variance versus model complexity in a single plot, like done [here](https://elitedatascience.com/bias-variance-tradeoff).

- As model complexity increases, bias decreases but variance increases.
- The goal is to achieve low bias and low variance. The optimal model complexity minimizes the total error.

## Implications for Different ML Algorithms

The bias-variance tradeoff manifests differently in different ML algorithms [[2](#ref-2)] [[4](#ref-4)]:

- Linear algorithms like linear regression have high bias but low variance
- Non-linear algorithms like decision trees have low bias but high variance
- Deep learning models are more complex and can reduce bias without increasing variance as much, but they still exhibit some tradeoff [[1](#ref-1)] [[10](#ref-10)]

Tuning hyperparameters like regularization strength, tree depth, or number of neighbors can adjust the bias-variance balance for a given algorithm [[2](#ref-2)] [[4](#ref-4)].

## Mitigating the Tradeoff

Some techniques can help reduce bias and variance simultaneously:

- Using more training data reduces variance without impacting bias [[11](#ref-11)] [[12](#ref-12)]
- Cross-validation helps assess generalization performance and tune model complexity [[12](#ref-12)]
- Ensemble methods like bagging and boosting can reduce variance [[12](#ref-12)]
- Regularization techniques constrain model complexity to reduce overfitting [[4](#ref-4)] [[12](#ref-12)]

However, the irreducible error due to noise in the data provides a lower bound on the total error that cannot be overcome [[16](#ref-16)].

In summary, the bias-variance tradeoff is a key concept to understand when developing ML models. The goal is to find the right model complexity to minimize both bias and variance and achieve the best generalization performance. Different algorithms have different tradeoffs, but techniques like cross-validation, regularization, and ensembles can help strike the right balance.

## Mathematical derivation of bias-variance decomposition

Let $$f(x)$$ be the true underlying function we are trying to learn. We assume $$f$$ is fixed but unknown. Let $$y$$ be the observed target variable which is related to $$f(x)$$ by:

$$
y = f(x) + \epsilon
$$

where $$\epsilon$$ is random noise with mean zero and variance $$\sigma^2_\epsilon$$. That is:

$$
\mathbb{E}[\epsilon] = 0, \quad \mathbb{E}[\epsilon^2] = \sigma^2_\epsilon
$$

Let $$\hat{f}(x)$$ be the function learned by our model from a finite training set $$\mathcal{D}$$. Note that $$\hat{f}$$ is a random variable, since it depends on the randomness in $$\mathcal{D}$$. 

The expected squared prediction error of $$\hat{f}$$ at a point $$x$$ is:

$$
\mathbb{E}_{\mathcal{D},\epsilon}\left[(y - \hat{f}(x))^2\right] = \mathbb{E}_{\mathcal{D},\epsilon}\left[(f(x) + \epsilon - \hat{f}(x))^2\right]
$$

Expanding the square and using linearity of expectation:

$$
\begin{align*}
\mathbb{E}_{\mathcal{D},\epsilon}\left[(f(x) + \epsilon - \hat{f}(x))^2\right] &= \mathbb{E}_{\mathcal{D},\epsilon}\left[f(x)^2 + \epsilon^2 + \hat{f}(x)^2 + 2f(x)\epsilon - 2f(x)\hat{f}(x) - 2\epsilon\hat{f}(x)\right] \\
&= f(x)^2 + \mathbb{E}[\epsilon^2] + \mathbb{E}_\mathcal{D}[\hat{f}(x)^2] + 2f(x)\mathbb{E}[\epsilon] - 2f(x)\mathbb{E}_\mathcal{D}[\hat{f}(x)] - 2\mathbb{E}_\mathcal{D}[\hat{f}(x)]\mathbb{E}[\epsilon] \\
&= f(x)^2 + \sigma^2_\epsilon + \mathbb{E}_\mathcal{D}[\hat{f}(x)^2] - 2f(x)\mathbb{E}_\mathcal{D}[\hat{f}(x)]
\end{align*}
$$

In the last step, we used the fact that $$\mathbb{E}[\epsilon]=0$$ and that $$\epsilon$$ is independent of $$\hat{f}$$ so $$\mathbb{E}_\mathcal{D}[\hat{f}(x)]\mathbb{E}[\epsilon]=0$$.

Now, let's add and subtract $$\mathbb{E}_\mathcal{D}[\hat{f}(x)]^2$$ to get:

$$
\begin{align*}
\mathbb{E}_{\mathcal{D},\epsilon}\left[(y - \hat{f}(x))^2\right] &= f(x)^2 + \sigma^2_\epsilon + \mathbb{E}_\mathcal{D}[\hat{f}(x)^2] - 2f(x)\mathbb{E}_\mathcal{D}[\hat{f}(x)] + \mathbb{E}_\mathcal{D}[\hat{f}(x)]^2 - \mathbb{E}_\mathcal{D}[\hat{f}(x)]^2 \\
&= \sigma^2_\epsilon + \left(\mathbb{E}_\mathcal{D}[\hat{f}(x)]^2 - 2f(x)\mathbb{E}_\mathcal{D}[\hat{f}(x)] + f(x)^2\right) + \left(\mathbb{E}_\mathcal{D}[\hat{f}(x)^2] - \mathbb{E}_\mathcal{D}[\hat{f}(x)]^2\right) \\
&= \sigma^2_\epsilon + \left(\mathbb{E}_\mathcal{D}[\hat{f}(x)] - f(x)\right)^2 + \mathbb{E}_\mathcal{D}\left[\left(\hat{f}(x) - \mathbb{E}_\mathcal{D}[\hat{f}(x)]\right)^2\right]
\end{align*}
$$

The three terms in the final expression are:

1. $$\sigma^2_\epsilon$$ is the irreducible error due to the noise in the data. This cannot be reduced by any model.

2. $$\left(\mathbb{E}_\mathcal{D}[\hat{f}(x)] - f(x)\right)^2$$ is the squared bias, the amount by which the average prediction over all possible training sets differs from the true value.

3. $$\mathbb{E}_\mathcal{D}\left[\left(\hat{f}(x) - \mathbb{E}_\mathcal{D}[\hat{f}(x)]\right)^2\right]$$ is the variance, the expected squared deviation of $$\hat{f}(x)$$ around its mean.

Therefore, we have decomposed the expected prediction error into three parts: irreducible error, bias, and variance:

$$
\mathbb{E}_{\mathcal{D},\epsilon}\left[(y - \hat{f}(x))^2\right] = \sigma^2_\epsilon + \text{Bias}^2(\hat{f}(x)) + \text{Var}(\hat{f}(x))
$$

This is the bias-variance decomposition. It shows that to minimize the expected prediction error, we need to simultaneously minimize both the bias and variance. However, there is usually a tradeoff between bias and variance - models with high bias tend to have low variance and vice versa. The art of machine learning is to find the sweet spot that balances bias and variance to minimize the total prediction error.

---
## References

[1] <a id="ref-1"></a> [reddit.com: Do Successful Models Defy the Bias-Variance Tradeoff?](https://www.reddit.com/r/MachineLearning/comments/nkfarw/d_do_successful_models_defy_the_biasvariance/)  
[2] <a id="ref-2"></a> [machinelearningmastery.com: Gentle Introduction to the Bias-Variance Trade-Off in Machine Learning](https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/)  
[3] <a id="ref-3"></a> [towardsdatascience.com: Bias, Variance and How They Are Related to Underfitting, Overfitting](https://towardsdatascience.com/bias-variance-and-how-they-are-related-to-underfitting-overfitting-4809aed98b79)  
[4] <a id="ref-4"></a> [serokell.io: Bias-Variance Tradeoff Explained](https://serokell.io/blog/bias-variance-tradeoff)  
[5] <a id="ref-5"></a> [datascience.stackexchange.com: Relation Between Underfitting vs High Bias and Low Variance](https://datascience.stackexchange.com/questions/117189/relation-between-underfitting-vs-high-bias-and-low-variance)  
[6] <a id="ref-6"></a> [kdnuggets.com: Understanding the Bias-Variance Trade-off in 3 Minutes](https://www.kdnuggets.com/2020/09/understanding-bias-variance-trade-off-3-minutes.html)  
[7] <a id="ref-7"></a> [machinelearningcompass.com: Bias and Variance in Machine Learning](https://machinelearningcompass.com/model_optimization/bias_and_variance/)  
[8] <a id="ref-8"></a> [elitedatascience.com: Bias-Variance Tradeoff: Intuitive Explanation](https://elitedatascience.com/bias-variance-tradeoff)  
[9] <a id="ref-9"></a> [geeksforgeeks.org: Underfitting and Overfitting in Machine Learning](https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/)  
[10] <a id="ref-10"></a> [towardsdatascience.com: Examples of Bias-Variance Tradeoff in Deep Learning](https://towardsdatascience.com/examples-of-bias-variance-tradeoff-in-deep-learning-6420476a20bd)  
[11] <a id="ref-11"></a> [towardsdatascience.com: Understanding the Bias-Variance Tradeoff](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)  
[12] <a id="ref-12"></a> [mastersindatascience.org: The Difference Between Bias and Variance](https://www.mastersindatascience.org/learning/difference-between-bias-and-variance/)  
[13] <a id="ref-13"></a> [javatpoint.com: Bias and Variance in Machine Learning](https://www.javatpoint.com/bias-and-variance-in-machine-learning)  
[14] <a id="ref-14"></a> [geeksforgeeks.org: Bias-Variance Trade-Off in Machine Learning](https://www.geeksforgeeks.org/ml-bias-variance-trade-off/)  
[15] <a id="ref-15"></a> [cs.cornell.edu: Bias-Variance Decomposition](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html)  
[16] <a id="ref-16"></a> [wikipedia.org: Bias–Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)  
[17] <a id="ref-17"></a> [mlu-explain.github.io: Bias-Variance Tradeoff](https://mlu-explain.github.io/bias-variance/)  
[18] <a id="ref-18"></a> [shiksha.com: Bias and Variance in Machine Learning](https://www.shiksha.com/online-courses/articles/bias-and-variance/)  
[19] <a id="ref-19"></a> [pnas.org: Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off](https://www.pnas.org/doi/10.1073/pnas.1903070116)  
[20] <a id="ref-20"></a> [towardsai.net: Bias-Variance Decomposition 101: A Step-by-Step Computation](https://pub.towardsai.net/bias-variance-decomposition-101-a-step-by-step-computation-9d5f3694877?gi=6e9bfdf85e71)  
[21] <a id="ref-21"></a> [cs.toronto.edu: Lecture 5 - Decision Trees & Bias-Variance Decomposition](https://www.cs.toronto.edu/~rgrosse/courses/csc311_f20/slides/lec05.pdf)  
[22] <a id="ref-22"></a> [jmlr.org: Bias–Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods](https://www.jmlr.org/papers/volume5/valentini04a/valentini04a.pdf)  
[23] <a id="ref-23"></a> [allenkunle.me: Bias-Variance Decomposition](https://allenkunle.me/bias-variance-decomposition)  
[24] <a id="ref-24"></a> [stanford.edu: Bias-Variance Analysis](https://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf)  
[25] <a id="ref-25"></a> [cs.toronto.edu: Lecture 2 - Linear Regression & Bias-Variance Decomposition](https://www.cs.toronto.edu/~rgrosse/courses/csc311_f21/lectures/lec02.pdf)  
[26] <a id="ref-26"></a> [oregonstate.edu: Bias–Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods](https://web.engr.oregonstate.edu/~tgd/publications/jmlr-valentini-bv.pdf)  
[27] <a id="ref-27"></a> [scikit-learn.org: Single Estimator versus Bagging: Bias-Variance Decomposition](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html)  
[28] <a id="ref-28"></a> [rasbt.github.io: Bias-Variance Decomposition](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)  
[29] <a id="ref-29"></a> [stats.stackexchange.com: Understanding Bias-Variance Tradeoff Derivation](https://stats.stackexchange.com/questions/204115/understanding-bias-variance-tradeoff-derivation)  
[30] <a id="ref-30"></a> [berkeley.edu: Linear Regression and the Bias-Variance Tradeoff](https://people.eecs.berkeley.edu/~jegonzal/assets/slides/linear_regression.pdf)  
[31] <a id="ref-31"></a> [cmu.edu: Bias-Variance Decomposition](https://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf)  
[32] <a id="ref-32"></a> [cornell.edu: Bias-Variance Tradeoff and Ridge Regression](https://www.cs.cornell.edu/courses/cs4780/2023fa/slides/Bias_Var_Ridge.pdf)  
[33] <a id="ref-33"></a> [oregonstate.edu: Bias-Variance Tradeoff](https://web.engr.oregonstate.edu/~tgd/classes/534/slides/part9.pdf)  
[34] <a id="ref-34"></a> [towardsdatascience.com: Bias and Variance for Model Assessment](https://towardsdatascience.com/bias-and-variance-for-model-assessment-a2edb69d097f)  
[35] <a id="ref-35"></a> [wikipedia.org: Bias–Variance Tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)  
[36] <a id="ref-36"></a> [towardsdatascience.com: The Bias-Variance Tradeoff](https://towardsdatascience.com/the-bias-variance-tradeoff-8818f41e39e9)  
[37] <a id="ref-37"></a> [stats.stackexchange.com: Bias-Variance Tradeoff with SVMs](https://stats.stackexchange.com/questions/240503/bias-variance-tradeoff-with-svms)  
[38] <a id="ref-38"></a> [youtube.com: 12 - Bias-Variance Tradeoff](https://www.youtube.com/watch?v=zUJbRO0Wavo)  

_Based on a chat with claude-3-opus on [perplexity.ai](https://perplexity.ai)_

<!-- -------------------------------------------------------------- -->
<!-- 
sequence: renumber, accumulate, format

to increment numbers, use multiple cursors then emmet shortcuts

regex...
\[(\d+)\]
to
 [[$1](#ref-$1)]

regex...
\[(\d+)\] (.*)
to
[$1] <a id="ref-$1"></a> [display text]($2)  

change "Citations:" to "## References"
-->
<!-- 
Include images like this:  
<figure style="text-align: center; width:100%;">
    <img src="{{site.baseurl}}/images/experimenting_files/experimenting_18_1.svg" alt="___" style="max-width:90%; 
    height: auto; margin:3% auto; display:block;">
    <figcaption>___</figcaption>
</figure> 
-->
<!-- 
Include code snippets like this:  
```python 
def square(x):
    return x**2
``` 
-->
<!-- 
Cite like this [[2](#ref-2)], and this [[3](#ref-3)]. Use two extra spaces at end of each line for line break
---
## References  
[1] <a id="ref-1"></a> [display text](hyperlink)  
[2] <a id="ref-2"></a> [display text](hyperlink) 
[3] <a id="ref-3"></a> [display text](hyperlink)  
_Assisted by claude-3-opus on [perplexity.ai](https://perplexity.ai)_ 
-->
<!-- -------------------------------------------------------------- -->