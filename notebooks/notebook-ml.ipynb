{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An introduction to Pytorch and JAX\n",
    "\n",
    "Cite like this [[1](#ref-1)]. \n",
    "\n",
    "In this post, we hope to concisely introduce two prominent frameworks for deep learning: Pytorch and JAX. We also hoped to cover Tensorflow. However for reasons unclear to us, we were unable to install Tensorflow both with Python 3.12 and Python 3.11. Given the simultaneous development of Tensorflow and JAX within Google, and the recent popularity of JAX, I wouldn't be surprised if Tensorflow is heading towards deprecation in the future.\n",
    "\n",
    "Pytorch was first developed in 2016 by Adam Paszke and Soumith Chintala along with others at FAIR [[1](#ref-1)]. Some unique features of Pytorch include: dynamic computation graph, pythonic nature, and extensive ecosystem (notably torchvision, torchaudio, and torchtext)\n",
    "\n",
    "JAX (\"just after execution\") was first developed in 2018 by Roy Frostig, Matthew James Johnson, and Chris Leary at Google Brain [[2](#ref-2)]. Some unique features of JAX include: jit compilation (\"just in time compilation\"), XLA (\"accelerated linear algebra\"), autovectorization & large data parallelism (via `vmap` and `pmap` respectively) \n",
    "\n",
    "What both Pytorch and JAX have in common is automatic differentiation (`autograd` in pytorch, and just `grad` in JAX). The execution speed however is faster in JAX since it benefits from autovectorization and jit compilation abilities mentioned earlier. On the other hand, what makes Pytorch and JAX fundamentally different as frameworks is the programming paradigm they use: Pytorch is object-oriented, while JAX is functional.\n",
    "\n",
    "Let us look at some examples. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ### PyTorch\n",
    "Developed by FAIR, PyTorch is celebrated for its ease of use, flexibility, and dynamic computation graph. It allows for intuitive model development and debugging, making it a favorite among researchers and developers for prototyping and experimentation.\n",
    "**Unique Features:**\n",
    "- **Dynamic Computation Graph:** PyTorch constructs the computation graph on-the-fly. This feature, known as eager execution, allows for more interactive and dynamic model development [14].\n",
    "- **Pythonic Nature:** PyTorch's design is highly pythonic, making it easy to learn and use, especially for those already familiar with Python [14].\n",
    "- **Extensive Ecosystem:** With libraries like torchvision, torchaudio, and torchtext, PyTorch provides a rich ecosystem for various applications, from computer vision to natural language processing [14].\n",
    "### JAX\n",
    "JAX, developed by Google, is a framework that extends NumPy with automatic differentiation and GPU/TPU acceleration. It is particularly suited for high-performance machine learning research and scientific computing.\n",
    "**Unique Features:**\n",
    "- **Functional Programming Style:** JAX encourages pure functions and immutability, aligning with functional programming principles. This approach can simplify certain types of mathematical modeling and transformation composition [12][13]. -->\n",
    "<!-- - **Composable Transformations:** JAX provides powerful function transformations like `grad`, `jit`, and `vmap` for differentiation, just-in-time compilation, and vectorization, respectively [12][13].\n",
    "- **Performance:** Leveraging XLA (Accelerated Linear Algebra) for compiling and optimizing computations, JAX can achieve significant performance improvements, especially on accelerators like GPUs and TPUs [13][18].\n",
    "## Summary Comparison\n",
    "- **Ease of Use:** PyTorch is often praised for its intuitive and pythonic API, making it easier for beginners and for rapid prototyping. JAX, with its functional programming model, might have a steeper learning curve but offers powerful capabilities for those familiar with its paradigm [14][18].\n",
    "- **Performance:** JAX is designed for high-performance computing, leveraging JIT compilation and XLA. PyTorch also offers performance optimizations but might not match JAX's speed in certain scenarios, especially on TPUs [13][18].\n",
    "- **Ecosystem and Community:** PyTorch benefits from a larger community and a more extensive ecosystem of libraries and tools. JAX, while having a smaller ecosystem, is rapidly growing and includes specialized libraries like Flax and Haiku for neural network development [14][18].\n",
    "In conclusion, the choice between PyTorch and JAX is a trade-off between versatility and performance optimization. PyTorch is considered more accessible and versatile for a wide range of applications, while JAX is well-known for its performance optimization and functional programming features - suitable for large-scale ML and scientific computing in particular [][] -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  \n",
    "[1] <a id=\"ref-1\"></a> [https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf)  \n",
    "[2] <a id=\"ref-2\"></a> [https://mlsys.org/Conferences/doc/2018/146.pdf](https://mlsys.org/Conferences/doc/2018/146.pdf)  \n",
    "[3] <a id=\"ref-3\"></a> []()  \n",
    "[4] <a id=\"ref-4\"></a> []()  \n",
    "[5] <a id=\"ref-5\"></a> []()  \n",
    "<!-- use two extra spaces at end of each line for line break -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
