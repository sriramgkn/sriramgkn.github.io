{"cells":[{"cell_type":"markdown","metadata":{"id":"McCMwsiJo7hs"},"source":["## Understanding backpropogation"]},{"cell_type":"markdown","metadata":{"id":"lxc4FTzKo7ht"},"source":["In this notebook, we understand backpropogation using elementary arithmetic operations following [Andrej Karpathy's tutorial](https://www.youtube.com/watch?v=VMj-3S1tku0). We recommend visiting [micrograd](https://github.com/karpathy/micrograd) on github."]},{"cell_type":"markdown","metadata":{"id":"wbBDqpOwo7ht"},"source":["We start with a few basic imports:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hT7XUHH4o7ht"},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"e3Z3PMpYo7hu"},"source":["We now define the Value class which forms the heart of micrograd:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n14ECG26o7hu"},"outputs":[],"source":["class Value:\n","    \"\"\" stores a single scalar value and its gradient \"\"\"\n","    def __init__(self, data, _children=(), _op='', label=''): #__init__(object, data, attributes)\n","        self.data = data\n","        self.grad = 0.0\n","        self._backward = lambda: None\n","        self._prev = set(_children)\n","        self._op = _op\n","        self.label = label\n","    def __repr__(self):\n","        return f\"Value(data={self.data})\"\n","\n","    def __add__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data + other.data, (self, other), '+')\n","        def _backward():\n","            self.grad += 1.0 * out.grad\n","            other.grad += 1.0 * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def __mul__(self, other):\n","        other = other if isinstance(other, Value) else Value(other)\n","        out = Value(self.data * other.data, (self, other), '*')\n","        def _backward():\n","            self.grad += other.data * out.grad\n","            other.grad += self.data * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def __pow__(self, other):\n","        assert isinstance(other, (int, float)) # only supporting int/float powers for now\n","        out = Value(self.data**other, (self,), f'**{other}')\n","\n","        def _backward():\n","            self.grad += (other * self.data**(other-1)) * out.grad\n","        out._backward = _backward\n","\n","        return out\n","\n","    def __truediv__(self, other): # self / other\n","        return self*other**-1\n","    def __neg__(self): # -self\n","        return self * -1\n","    def __sub__(self, other): # self - other\n","        return self + (-other)\n","\n","    def __rmul__(self, other): # other * self\n","        return self * other\n","    def __radd__(self, other): # other + self\n","        return self + other\n","    def __rsub__(self, other): # other - self\n","        return other + (-self)\n","    def __rtruediv__(self, other): # other / self\n","        return other * self**-1\n","\n","    def tanh(self):\n","        x = self.data\n","        t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n","        out = Value(t, (self,), 'tanh')\n","        def _backward():\n","            self.grad += (1 - t**2) * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    def exp(self):\n","        x = self.data\n","        out = Value(math.exp(x), (self,), 'exp')\n","        def _backward():\n","            self.grad += out.data * out.grad\n","        out._backward = _backward\n","        return out\n","\n","    #pow, truediv\n","\n","    def backward(self):\n","        #topological sort\n","        topo = []\n","        visited = set()\n","        def build_topo(v):\n","            if v not in visited:\n","                visited.add(v)\n","                for child in v._prev:\n","                    build_topo(child)\n","                topo.append(v)\n","        self.grad = 1.0\n","        build_topo(self)\n","        for node in reversed(topo):\n","            node._backward()\n","        # implements in order: o._backward(), nn._backward(), x1w1x2w2._backward(), x2w2._backward(), x1w1._backward()\n"]},{"cell_type":"markdown","metadata":{"id":"AQdMt8Wlo7hu"},"source":["Basically, the Value class defines python's elementary arithmetic operations (like add, multiply, divide) for a computational graph represented as a tree data structure: the root node of the tree signifies the neural net output, and leaf nodes signify the input data vector along with first-layer weights. In addition, it includes rules for local gradient updates in $\\texttt{\\_backward()}$ and a full-graph gradient update in $\\texttt{backward()}$. The full-graph gradient update combines $\\texttt{\\_backward()}$ with a [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) approach to traversing the graph from output to input nodes. The main non-elementary differentiable operation it contains is [tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions), which is to be used as the nonlinear activation function at each node."]},{"cell_type":"markdown","metadata":{"id":"GHgOVb_3o7hu"},"source":["We now include a graphviz code like Andrej does, that is capable of visualizing such a computational graph diagrammatically"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fywxjs44o7hv"},"outputs":[],"source":["from graphviz import Digraph\n","def trace(root):\n","    nodes, edges = set(), set()\n","    def build(v):\n","        if v not in nodes:\n","            nodes.add(v)\n","            for child in v._prev:\n","                edges.add((child, v))\n","                build(child)\n","    build(root)\n","    return nodes, edges\n","\n","def draw_dot(root, format='svg', rankdir='LR'):\n","    \"\"\"\n","    format: png | svg | ...\n","    rankdir: TB (top to bottom graph) | LR (left to right)\n","    \"\"\"\n","    assert rankdir in ['LR', 'TB']\n","    nodes, edges = trace(root)\n","    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n","\n","    for n in nodes:\n","        dot.node(name=str(id(n)), label = \"{ %s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad), shape='record')\n","        if n._op:\n","            dot.node(name=str(id(n)) + n._op, label=n._op)\n","            dot.edge(str(id(n)) + n._op, str(id(n)))\n","\n","    for n1, n2 in edges:\n","        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n","\n","    return dot"]},{"cell_type":"markdown","metadata":{"id":"ugilvhV2o7hv"},"source":["The code above can basically draw the entire graph, and indicate label, data, \\& gradient information at each node. The gradient at each node $j$ (as computed in value class) is the partial derivative of its weight $w_j$ w.r.t. the output loss function $L(o)$, so that's $\\frac{\\partial w_j}{\\partial L}$. As such the NN output is just $o(\\textbf{x}_{in},\\textbf{w})$; the loss function is an additional step computing a function of output and supervised output labels of training data, as a simple example $\\lVert o(\\textbf{x}_{in},\\textbf{w}) - l \\rVert_{2}$, where weight vector $\\textbf{w}$ is known to produce a scalar output $l$, and the 2-norm (mean squared loss) is only really sensible in the limit of multiple outputs (so that $\\textbf{o}$ and $\\textbf{l}$ become vectors)"]},{"cell_type":"markdown","metadata":{"id":"ZsmJvqdeo7hv"},"source":["Just out of curiosity, the hyperbolic tangent is plotted to visualize its nonlinearity:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9OpG4AoUo7hv"},"outputs":[],"source":["plt.plot( np.arange(-5,5,0.2), np.tanh(np.arange(-5,5,0.2)) ); plt.grid()"]},{"cell_type":"markdown","metadata":{"id":"KTIrmnAOo7hv"},"source":["In what follows, a simple two-input neuron is initialized via the value class:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkigL2rio7hv"},"outputs":[],"source":["x1 = Value(2.0,label='x1')\n","x2 = Value(0.0,label='x2')\n","w1 = Value(-3.0,label='w1')\n","w2 = Value(1.0,label='w2')\n","b = Value(6.88137358,label='b')\n","x1w1 = x1*w1; x1w1.label = 'x1*w1'; print(x1w1)\n","x2w2 = x2*w2; x2w2.label = 'x2*w2'; print(x2w2)\n","x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'; print(x1w1x2w2)\n","nn = x1w1x2w2 + b; nn.label = 'nn'; print(nn)\n","o = nn.tanh(); o.label = 'o'; print(o)"]},{"cell_type":"markdown","metadata":{"id":"Y3OJ2grPo7hw"},"source":["and then the gradients are backpropogated using the $\\texttt{backward()}$ function in value class, and visualized using the $\\texttt{draw\\_dot()}$ function defined in the graphviz section earlier:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P55lZewSo7hw"},"outputs":[],"source":["o.backward()\n","draw_dot(o)"]},{"cell_type":"markdown","metadata":{"id":"EQ2H1mnqo7hw"},"source":["Now, just as an extra exercise in object oriented programming, the tanh function is further broken down as $\\tanh(x) = \\frac{e^{2x} + 1}{e^{2x} - 1}$ and we implement it as a composite function of exponentiation, addition (subtraction), division. Division in the value class is implemented more generally as a composite of multiplication and monomials: $\\frac{a}{b} = a*b^{-1}$, so implement $\\texttt{pow()}$ in value class so that we can more generally produce monomials $x^n$ of arbitrary degree in our arithmetic system."]},{"cell_type":"markdown","metadata":{"id":"5MkLXYfSo7hw"},"source":["Once the above steps are implemented in the value class, we can now produce a longer computational graph and verify that we still get the same gradients:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKuv0sYzo7hw"},"outputs":[],"source":["#after implementing pow, truediv, exp\n","# inputs x1, x2\n","x1 = Value(2.0,label='x1')\n","x2 = Value(0.0,label='x2')\n","# weights w1, w2\n","w1 = Value(-3.0,label='w1')\n","w2 = Value(1.0,label='w2')\n","# neuron bias\n","b = Value(6.88137358,label='b')\n","\n","x1w1 = x1 * w1; x1w1.label = 'x1*w1'; print(x1w1)\n","x2w2 = x2 * w2; x2w2.label = 'x2*w2'; print(x2w2)\n","x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'; print(x1w1x2w2)\n","nn = x1w1x2w2 + b; nn.label = 'nn'; print(nn)\n","ee = (2 * nn).exp(); print(ee)\n","o = (ee - 1) / (ee + 1)\n","o.backward()\n","draw_dot(o)"]},{"cell_type":"markdown","metadata":{"id":"an_zLtsAo7hw"},"source":["We now shift focus to being able to achieve the same with Pytorch: the most popular deep learning library. In the process, we can verify the functional equivalence of micrograd's and Pytorch's $\\texttt{backward()}$ functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_V1qWiRo7hw"},"outputs":[],"source":["# single two-param neuron backprop in pytorch\n","\n","import torch\n","x1 = torch.Tensor([2.0]).double()   ;   x1.requires_grad = True\n","x2 = torch.Tensor([0.0]).double()   ;   x2.requires_grad = True\n","w1 = torch.Tensor([-3.0]).double()  ;   w1.requires_grad = True\n","w2 = torch.Tensor([1.0]).double()   ;   w2.requires_grad = True\n","b = torch.Tensor([6.8813735]).double() ;   b.requires_grad = True\n","n = x1*w1 + x2*w2 + b\n","o = torch.tanh(n)\n","\n","print(o.data.item())\n","o.backward()\n","\n","print('---')\n","print('x2', x2.grad.item())\n","print('w2', w2.grad.item())\n","print('x1', x1.grad.item())\n","print('w1', w1.grad.item())\n"]},{"cell_type":"markdown","metadata":{"id":"Nw2PzfNuo7hw"},"source":["That's it from me! I'm skipping the part where Andrej discusses the $\\texttt{nn.py}$ classes, as that's primarily linguistic abstractions. The core mathematics is automatic differentiation on computational graphs. Backpropogation is simply a special case where such an autograd engine is applied to an \"ansatz\" of nonlinear functions we call neural networks. Have a wonderful day!"]},{"cell_type":"markdown","metadata":{"id":"mt2HGUQJo7hw"},"source":["Roughwork"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFayqW2co7hw"},"outputs":[],"source":["a = Value(2.0)\n","b = Value(4.0)\n","a / b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zd9qYzDAo7hw"},"outputs":[],"source":["a = Value(3.0, label='a')\n","b=a+a; b.label = 'b'\n","b.backward()\n","draw_dot(b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOQPtGayo7hw"},"outputs":[],"source":["a = Value(-2.0, label='a')\n","b = Value(3.0, label='b')\n","d = a * b   ;   d.label = 'd'\n","e = a + b   ;   e.label = 'e'\n","f = d * e   ;   f.label = 'f'\n","f.backward()\n","draw_dot(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IeWwKgQo7hx"},"outputs":[],"source":["a = Value(2.0)\n","2 * a\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"https://github.com/sriramgkn/micrograd-sri/blob/main/experimenting.ipynb","timestamp":1710045283877}]}},"nbformat":4,"nbformat_minor":0}